{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import config\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import datetime\n",
    "import glob\n",
    "import time\n",
    "import uuid\n",
    "import csv\n",
    "import gradio as gr\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.exceptions import AzureError\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from dotenv import dotenv_values\n",
    "from azure.cosmos import ThroughputProperties\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"argus.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "# Azure Cosmos DB connection details\n",
    "HOST = config['cosmos_host']\n",
    "MASTER_KEY = config['cosmos_key']\n",
    "\n",
    "cosmos_connection_string = config['cosmos_string']\n",
    "\n",
    "container_name = \"ChatMessages\"\n",
    "\n",
    "\n",
    "# Azure OpenAI connection details\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_version = config['openai_version']\n",
    "openai_embeddings_model = config['openai_embeddings_deployment']\n",
    "openai_chat_model = config['AZURE_OPENAI_CHAT_MODEL']\n",
    "\n",
    "# dalle = config['Dall-e']  \n",
    "\n",
    "dbsource = config['cosmosdbsourcedb'] \n",
    "colvector = config['cosmosdbsourcecol']\n",
    "cachecol = config['cosmsodbcache']\n",
    "cosmosdbcolcompletion = config['cosmosdbcolcompletion']\n",
    "container_name = config['cosmosdbcolcompletion']\n",
    "\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "  api_key = openai_key,  \n",
    "  api_version = openai_version,  \n",
    "  azure_endpoint =openai_endpoint \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#ENDPOINT =  config['cosmos_host']\n",
    "#credential = DefaultAzureCredential()\n",
    "\n",
    "#client = CosmosClient(ENDPOINT, credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createvectordb(collection):\n",
    "    \n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(dbsource)\n",
    "    \n",
    "    print(collection)\n",
    "    \n",
    "    vector_embedding_policy = { \"vectorEmbeddings\": [ {  \"path\": \"/embedding\",  \"dataType\": \"float32\",  \"distanceFunction\": \"cosine\",  \"dimensions\": 1536  } ] }\n",
    "    indexing_policy = { \"includedPaths\": [ { \"path\": \"/*\" } ], \"excludedPaths\": [  {  \"path\": \"/\\\"_etag\\\"/?\" }  ], \"vectorIndexes\": [ {\"path\": \"/embedding\", \"type\": \"diskANN\"  }] }\n",
    "\n",
    "    try:\n",
    "        container = mydbt.create_container_if_not_exists( \n",
    "        id= collection, \n",
    "        partition_key=PartitionKey(path='/id'), \n",
    "        indexing_policy=indexing_policy, \n",
    "        vector_embedding_policy=vector_embedding_policy) \n",
    "        print(\"container created\")\n",
    "              \n",
    "    except : \n",
    "         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container with id '<built-in function id>' created\n"
     ]
    }
   ],
   "source": [
    "from azure.cosmos import exceptions, PartitionKey\n",
    "\n",
    "\n",
    "client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "\n",
    "mydbt = client.create_database_if_not_exists(dbsource)\n",
    "\n",
    "mydbt = client.get_database_client(dbsource)\n",
    "    \n",
    "# Create the vector embedding policy to specify vector details\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [ \n",
    "        { \n",
    "            \"path\":\"/vector\" ,\n",
    "             \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"dotproduct\",\n",
    "            \"dimensions\":1536\n",
    "        }, \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the vector index policy to specify vector details\n",
    "indexing_policy = {\n",
    "    \"vectorIndexes\": [ \n",
    "        {\n",
    "            \"path\": \"/vector\", \n",
    "            \"type\": \"diskANN\" \n",
    "        }\n",
    "    ]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the cache collection with vector index\n",
    "try:\n",
    "    cache_container =  mydbt.create_container_if_not_exists( id=cachecol, \n",
    "                                                  partition_key=PartitionKey(path='/id'), \n",
    "                                                  indexing_policy=indexing_policy,\n",
    "                                                  vector_embedding_policy=vector_embedding_policy\n",
    "                                                  ) \n",
    "    print('Container with id \\'{0}\\' created'.format(id)) \n",
    "\n",
    "except exceptions.CosmosHttpResponseError: \n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(openai_client, text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the OpenAI API v1.x\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating embeddings for: \", text, \" with model: \", openai_embeddings_model)\n",
    "    response = openai_client.embeddings.create(\n",
    "        input = text,\n",
    "        model= openai_embeddings_model\n",
    "        \n",
    "    )\n",
    "    \n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(db,collection) :\n",
    "    \n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(db)   \n",
    "  \n",
    "    print(\"Creating container...\"   )\n",
    "    try:\n",
    "        container = mydbt.create_container_if_not_exists( \n",
    "        id= collection, \n",
    "        partition_key=PartitionKey(path='/id')\n",
    "        )\n",
    "        query = \"SELECT * FROM c\"\n",
    "        source = mydbt.get_container_client(\"documents\")\n",
    "        result = source.query_items(\n",
    "            query=query,\n",
    "            enable_cross_partition_query=True)\n",
    "\n",
    "        for item in result:\n",
    "            item['text']= json.dumps(item)\n",
    "            container.upsert_item(item)\n",
    "\n",
    "    \n",
    "        \n",
    "# count products\n",
    "        query = \"SELECT VALUE COUNT(1) FROM c\"\n",
    "\n",
    "        total_count = 0\n",
    "\n",
    "        result = container.query_items(\n",
    "            query=query,\n",
    "            enable_cross_partition_query=True)\n",
    "\n",
    "        for item in result:\n",
    "            total_count += item\n",
    "\n",
    "        print(\"Total count:\", total_count)\n",
    "        \n",
    "       \n",
    "    \n",
    "    except : \n",
    "     raise  \n",
    "       \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc(openai_client, collection, doc,name):\n",
    "   \n",
    "  \n",
    "  \n",
    "    try:\n",
    "        doc1 = {}\n",
    "        doc1[\"id\"] = doc[\"id\"]\n",
    "        doc1[\"source\"]= name\n",
    "        doc1[\"embedding\"] = generate_embeddings(openai_client, json.dumps(doc))\n",
    "        \n",
    "        print(doc[\"id\"])\n",
    "        \n",
    "        collection.upsert_item(doc1)\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_completion(openai_client, model, prompt: str):    \n",
    "   \n",
    "    print(\"Prompt: \", prompt)\n",
    "   \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages =   prompt,\n",
    "        temperature = 0.1\n",
    "    )   \n",
    "   \n",
    "    return response.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(openai_client, db, query_text, limit):\n",
    "    \"\"\" \n",
    "        Get similar documents from Cosmos DB for NoSQL \n",
    "\n",
    "        input: \n",
    "            container: name of the container\n",
    "            query_text: user question\n",
    "            limit: max number of documents to return\n",
    "        output:\n",
    "            documents: json documents similar to the user question\n",
    "            elapsed_time\n",
    "    \"\"\"\n",
    "    # vectorize the question\n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(db)   \n",
    "    cvector = mydbt.get_container_client(colvector)\n",
    "    sim =0.78\n",
    "   \n",
    "    query_vector = generate_embeddings(openai_client, query_text)\n",
    "  \n",
    "     \n",
    "     \n",
    "    query = f\"\"\"\n",
    "        SELECT TOP @num_results  c.id,c.source, VectorDistance(c.embedding, @embedding) as SimilarityScore \n",
    "        FROM c\n",
    "        WHERE VectorDistance(c.embedding,@embedding) > @similarity_score\n",
    "        ORDER BY VectorDistance(c.embedding,@embedding)\n",
    "    \"\"\"\n",
    "    results = cvector.query_items(\n",
    "        query=query,\n",
    "         parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": query_vector},\n",
    "            {\"name\": \"@num_results\", \"value\": limit},\n",
    "            {\"name\": \"@similarity_score\", \"value\": sim}\n",
    "        ],\n",
    "        enable_cross_partition_query=True, populate_query_metrics=True\n",
    "    )   \n",
    "    \n",
    "           \n",
    "    listid = []\n",
    "    source = \"\"\n",
    "    # get products from list of id\n",
    "    id_list = [id for id in results]\n",
    "\n",
    "    for i in id_list:\n",
    "            listid.append(i['id'])\n",
    "            source = (i['source'])\n",
    "                                  \n",
    "            \n",
    "        \n",
    "        \n",
    " \n",
    "    \n",
    "    if listid == []:\n",
    "        products = []\n",
    "    else : \n",
    "        id_list_str = ', '.join([f\"'{id}'\" for id in listid]) \n",
    "        \n",
    "      \n",
    "        mycolt = mydbt.get_container_client(source)\n",
    "            \n",
    "        query = f\"\"\"\n",
    "                    SELECT * FROM c \n",
    "                    WHERE  c.id IN ({id_list_str})\n",
    "                \"\"\"\n",
    "                \n",
    "            \n",
    "        results = mycolt.query_items(\n",
    "                    query=query,\n",
    "                    enable_cross_partition_query=True\n",
    "        )\n",
    "\n",
    "        products = []\n",
    "        for product in results:\n",
    "            products.append(product)    \n",
    "\n",
    "          \n",
    "    \n",
    "\n",
    "    return products\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFeed(collection):\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "        mydbt = client.get_database_client(dbsource)   \n",
    "        mycolt = mydbt.get_container_client(collection)\n",
    "        mycoltembed = mydbt.get_container_client(\"vector\") \n",
    "        name = collection\n",
    "        \n",
    "     \n",
    "        # Define a point in time to start reading the feed from\n",
    "        time = datetime.datetime.now()\n",
    "        \n",
    "        print (time)\n",
    "        time = time - datetime.timedelta(days=1)\n",
    "        print (time)\n",
    "        \n",
    "           \n",
    "        \n",
    "        response = mycolt.query_items_change_feed(start_time=time)\n",
    "        #response = mycolt.query_items_change_feed( )\n",
    "        \n",
    "        for doc in response:\n",
    "            add_doc(openai_client, mycoltembed, doc,name)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_history( completions=3):\n",
    "    \n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(dbsource)   \n",
    "    container = mydbt.get_container_client(cachecol)\n",
    "    \n",
    "    results = container.query_items(\n",
    "        query= '''\n",
    "        SELECT TOP @completions *\n",
    "        FROM c\n",
    "        ORDER BY c._ts DESC\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@completions\", \"value\": completions},\n",
    "        ], enable_cross_partition_query=True)\n",
    "    results = list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_search( vectors, similarity_score = 0.99 , num_results=5):\n",
    "    # Execute the query\n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(dbsource)   \n",
    "    container = mydbt.get_container_client(cachecol)\n",
    "    \n",
    "    results = container.query_items(\n",
    "        query= '''\n",
    "        SELECT TOP @num_results  c.completion, VectorDistance(c.vector, @embedding) as SimilarityScore \n",
    "        FROM c\n",
    "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score \n",
    "        ORDER BY VectorDistance(c.vector,@embedding)\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": vectors},\n",
    "            {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            {\"name\": \"@similarity_score\", \"value\": similarity_score}\n",
    "        ],\n",
    "        enable_cross_partition_query=True, populate_query_metrics=True)\n",
    "   \n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        print(\"result query\")\n",
    "        print(result)\n",
    "        formatted_results.append(result)\n",
    "\n",
    "  \n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacheresponse(user_prompt, prompt_vectors, response):\n",
    "    \n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(dbsource)   \n",
    "    container = mydbt.get_container_client(cachecol)\n",
    "    \n",
    "    print(\"Caching response for prompt: \", user_prompt)\n",
    "    print(\"Response: \", response)\n",
    "    \n",
    "    \n",
    "    # Create a dictionary representing the chat document\n",
    "    chat_document = {\n",
    "        'id':  str(uuid.uuid4()),  \n",
    "        'prompt': user_prompt,\n",
    "        'completion': response['choices'][0]['message']['content'],\n",
    "        'completionTokens': str(response['usage']['completion_tokens']),\n",
    "        'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "        'totalTokens': str(response['usage']['total_tokens']),\n",
    "        'model': response['model'],\n",
    "        'vector': prompt_vectors\n",
    "    }\n",
    "    # Insert the chat document into the Cosmos DB container\n",
    "    container.create_item(body=chat_document)\n",
    "    print(\"item inserted into cache.\", chat_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearcache ():\n",
    "   \n",
    "    client = CosmosClient(HOST, {'masterKey': MASTER_KEY})\n",
    "    mydbt = client.get_database_client(dbsource)   \n",
    "  \n",
    "    \n",
    "      \n",
    "# Create the vector embedding policy to specify vector details\n",
    "    vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [ \n",
    "        { \n",
    "            \"path\":\"/vector\" ,\n",
    "             \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"cosine\",\n",
    "            \"dimensions\":1536\n",
    "        }, \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the vector index policy to specify vector details\n",
    "    indexing_policy = { \n",
    "    \"vectorIndexes\": [ \n",
    "        {\n",
    "            \"path\": \"/vector\", \n",
    "            \"type\": \"diskANN\"\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "    } \n",
    "   \n",
    "    mydbt.delete_container(cachecol)\n",
    "\n",
    "\n",
    "# Create the cache collection with vector index\n",
    "    try:\n",
    "        mydbt.create_container_if_not_exists( id=cachecol, \n",
    "                                                  partition_key=PartitionKey(path='/id'), \n",
    "                                                  indexing_policy=indexing_policy,\n",
    "                                                  vector_embedding_policy=vector_embedding_policy\n",
    "                                                ) \n",
    "        print('Container with id \\'{0}\\' created'.format(id)) \n",
    "\n",
    "    except exceptions.CosmosHttpResponseError: \n",
    "        raise \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatecompletionede(user_prompt, vector_search_results, chat_history):\n",
    "    \n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for ypurdata . You are designed to provide helpful answers to user questions about  your data.\n",
    "    You are friendly, helpful, and informative and can be lighthearted. Be concise in your responses, but still friendly.\n",
    "        - Only answer questions related to the information provided below. \n",
    "        - Write two lines of whitespace between each answer in the list.\n",
    "    '''\n",
    "\n",
    "    # Create a list of messages as a payload to send to the OpenAI Completions API\n",
    "\n",
    "    # system prompt\n",
    "    \n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "    \n",
    "    #chat history\n",
    "    for chat in chat_history:\n",
    "        messages.append({'role': 'user', 'content': chat['prompt'] + \" \" + chat['completion']})\n",
    "    \n",
    "    #user prompt\n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "\n",
    "    #vector search results\n",
    "    for result in vector_search_results:\n",
    "        messages.append({'role': 'system', 'content': result['text']})\n",
    "\n",
    "    \n",
    "    # Create the completion\n",
    "    response = get_completion(openai_client, openai_chat_model, messages)\n",
    "    print(\"Response from openai\", response)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(user_input):\n",
    "\n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(openai_client, user_input)\n",
    "    \n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = cache_search(vectors=user_embeddings , num_results=1)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "        return cache_results[0]['completion'], True\n",
    "    else:\n",
    "        # Perform vector search on the movie collection\n",
    "        print(\"\\n New result\\n\")\n",
    "        search_results = get_similar_docs(openai_client, dbsource, user_input, 3)\n",
    "        print(\"\\n search result done\\n\")\n",
    "        \n",
    "        \n",
    "        print(\"Getting Chat History\\n\")\n",
    "        # Chat history\n",
    "        chat_history = get_chat_history(1)\n",
    "\n",
    "        # Generate the completion\n",
    "        print(\"Generating completions \\n\")\n",
    "        completions_results = generatecompletionede(user_input, search_results, chat_history)\n",
    "\n",
    "        print(\"Caching response \\n\")\n",
    "        # Cache the response\n",
    "        cacheresponse(user_input, user_embeddings, completions_results)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # Return the generated LLM completion\n",
    "        return completions_results['choices'][0]['message']['content'], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#createvectordb(colvector)\n",
    "loaddata(dbsource,'argus')\n",
    "ReadFeed('argus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat genereric with your data \")\n",
    "     \n",
    "    msg = gr.Textbox(label=\"Ask me about Your  Data!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history, pk):\n",
    "        # Create a timer to measure the time it takes to complete the request\n",
    "        start_time = time.time()\n",
    "        # Get LLM completion\n",
    "        response_payload, cached = chat_completion(user_message)\n",
    "                \n",
    "        \n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = round((end_time - start_time) * 1000, 2)\n",
    "        response = response_payload\n",
    "        # Append user message and response to chat history\n",
    "        details = f\"\\n (Time: {elapsed_time}ms)\"\n",
    "        if cached:\n",
    "            details += \" (Cached)\"\n",
    "        chat_history.append([user_message, response_payload + details])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
